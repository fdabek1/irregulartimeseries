{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/so/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = 1\n",
    "events = {}\n",
    "times = {}\n",
    "\n",
    "for data_type in ['train', 'test']:\n",
    "    with open(DATA_FOLDER + 'event-' + str(file_index) + '-' + data_type + '.txt') as f_events:\n",
    "        events[data_type] = [[int(y) for y in x.split()] for x in f_events]\n",
    "        \n",
    "    with open(DATA_FOLDER + 'time-' + str(file_index) + '-' + data_type + '.txt') as f_times:\n",
    "        times[data_type] = [[float(y) for y in x.split()] for x in f_times]\n",
    "\n",
    "badge_map = {}\n",
    "with open(DATA_FOLDER + 'badges.csv') as f_badge_labels:\n",
    "    next(f_badge_labels)\n",
    "    for row in f_badge_labels:\n",
    "        id, name = row.split(',')\n",
    "        badge_map[int(id)] = name.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute deltas for times.\n",
    "\n",
    "It seems that the general approach usually is to just use timestamps, but going to also compute deltas because maybe will want to use that to make things easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = {}\n",
    "for data_type in ['train', 'test']:\n",
    "    deltas[data_type] = []\n",
    "    for seq in times[data_type]:\n",
    "        seq_deltas = []\n",
    "        for i in range(1, len(seq)):\n",
    "            seq_deltas.append(seq[i] - seq[i-1])\n",
    "        deltas[data_type].append(seq_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models for Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors for these models is RMSE:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Root-mean-square_deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Time Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Time \t 1,388,534,141.777\n",
      "Min Time \t 1,325,376,181.507\n",
      "Mean \t\t 1,359,084,129.1882882\n",
      "Std Dev \t 17,138,171.0850439\n",
      "\n",
      "---\n",
      "Top 10 largest times\n",
      "[1388534141.777, 1388534141.777, 1388534141.777, 1388534141.777, 1388534141.777, 1388533810.627, 1388533518.993, 1388533207.767, 1388532906.527, 1388532906.527]\n",
      "\n",
      "Top 10 smallest times\n",
      "[1325376181.507, 1325376183.3, 1325376480.743, 1325379503.237, 1325380415.083, 1325383721.89, 1325384932.437, 1325385234.173, 1325386138.81, 1325387337.333]\n"
     ]
    }
   ],
   "source": [
    "all_times = []\n",
    "\n",
    "for data_type in ['train', 'test']:\n",
    "    for seq in times[data_type]:\n",
    "        all_times += seq\n",
    "\n",
    "print('Max Time', '\\t', \"{:,}\".format(max(all_times)))\n",
    "print('Min Time', '\\t', \"{:,}\".format(min(all_times)))\n",
    "print('Mean', '\\t\\t', \"{:,}\".format(np.mean(all_times)))\n",
    "print('Std Dev', '\\t', \"{:,}\".format(np.std(all_times)))\n",
    "print('')\n",
    "\n",
    "sorted_times = sorted(all_times, reverse=True)\n",
    "print('---')\n",
    "print('Top 10 largest times')\n",
    "print(sorted_times[:10])\n",
    "\n",
    "sorted_times = sorted(all_times, reverse=False)\n",
    "print('')\n",
    "print('Top 10 smallest times')\n",
    "print(sorted_times[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Delta Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Delta \t 20,337,973.434000015\n",
      "Min Delta \t 0.013000011444091797\n",
      "Mean \t\t 826,231.5698269516\n",
      "Std Dev \t 1,043,219.0273143519\n",
      "\n",
      "---\n",
      "Top 10 largest deltas\n",
      "[20337973.434000015, 20061996.75300002, 19054174.663000107, 18746822.46299982, 18725172.24000001, 17476425.897000074, 17350034.782999992, 17132427.25999999, 16679536.736999989, 16436643.904000044]\n",
      "\n",
      "Top 10 smallest deltas\n",
      "[0.013000011444091797, 0.013000011444091797, 0.01399993896484375, 0.01399993896484375, 0.01399993896484375, 0.014000177383422852, 0.016000032424926758, 0.016000032424926758, 0.016000032424926758, 0.016000032424926758]\n"
     ]
    }
   ],
   "source": [
    "all_deltas = []\n",
    "\n",
    "for data_type in ['train', 'test']:\n",
    "    for seq in deltas[data_type]:\n",
    "        all_deltas += seq\n",
    "\n",
    "print('Max Delta', '\\t', \"{:,}\".format(max(all_deltas)))\n",
    "print('Min Delta', '\\t', \"{:,}\".format(min(all_deltas)))\n",
    "print('Mean', '\\t\\t', \"{:,}\".format(np.mean(all_deltas)))\n",
    "print('Std Dev', '\\t', \"{:,}\".format(np.std(all_deltas)))\n",
    "print('')\n",
    "\n",
    "sorted_deltas = sorted(all_deltas, reverse=True)\n",
    "print('---')\n",
    "print('Top 10 largest deltas')\n",
    "print(sorted_deltas[:10])\n",
    "\n",
    "sorted_deltas = sorted(all_deltas, reverse=False)\n",
    "print('')\n",
    "print('Top 10 smallest deltas')\n",
    "print(sorted_deltas[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_flat(arr):\n",
    "    flattened = []\n",
    "    for seq in arr:\n",
    "        flattened += seq\n",
    "        \n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "def compute_error(y_true, y_pred):\n",
    "    y_true = make_flat(y_true)\n",
    "    y_pred = make_flat(y_pred)\n",
    "    \n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def show_results(train_pred, test_pred):\n",
    "    print('----')\n",
    "    print('Train Error: ', compute_error(deltas['train'], train_pred))\n",
    "    print('Test Error: ', compute_error(deltas['test'], test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using methodology from https://github.com/dunan/NeuralPointProcess/blob/master/code/baselines/majority_predictor/time_mean_baseline.py\n",
    "\n",
    "A scale is used there so adding a variable for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean predictor 828.9236502234086\n",
      "----\n",
      "Train Error:  1332529.9103083226\n",
      "Test Error:  1321285.6966187225\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "num_train = 0\n",
    "for seq in times['train']:\n",
    "    for i in range(1, len(seq)):\n",
    "        total += scale * (seq[i] - seq[i-1])\n",
    "        num_train += 1\n",
    "\n",
    "mean_predictor = total / num_train\n",
    "print('Mean predictor', mean_predictor)\n",
    "\n",
    "train_pred = [[mean_predictor for d in seq[1:]] for seq in times['train']]\n",
    "test_pred = [[mean_predictor for d in seq[1:]] for seq in times['test']]\n",
    "show_results(train_pred, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying an alternate way by using the deltas to see if different results are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean predictor using deltas 828.923650223\n",
      "----\n",
      "Train Error:  1332529.9103083226\n",
      "Test Error:  1321285.6966187225\n"
     ]
    }
   ],
   "source": [
    "train_deltas = make_flat(deltas['train'])\n",
    "train_deltas = np.asarray(train_deltas) * scale\n",
    "\n",
    "mean_delta = np.mean(train_deltas)\n",
    "print('Mean predictor using deltas', mean_delta)\n",
    "\n",
    "train_pred = [[mean_delta for d in seq] for seq in deltas['train']]\n",
    "test_pred = [[mean_delta for d in seq] for seq in deltas['test']]\n",
    "show_results(train_pred, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "y = {}\n",
    "\n",
    "for data_type in ['train', 'test']:\n",
    "    x[data_type] = [seq[:-1] for seq in deltas[data_type]]\n",
    "    y[data_type] = [seq[1:] for seq in deltas[data_type]]\n",
    "\n",
    "    x[data_type] = np.asarray(x[data_type])\n",
    "    y[data_type] = np.asarray(y[data_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Seq Length 735\n"
     ]
    }
   ],
   "source": [
    "# Find the max user length\n",
    "user_lengths = [len(seq) for seq in deltas['train']] + [len(seq) for seq in deltas['test']]\n",
    "user_length_counts = {}\n",
    "for length in user_lengths:\n",
    "    if length not in user_length_counts:\n",
    "        user_length_counts[length] = 0\n",
    "        \n",
    "    user_length_counts[length] += 1\n",
    "user_lengths = list(set(user_lengths))\n",
    "user_lengths = sorted(user_lengths, reverse=True)\n",
    "max_user_length = max(user_lengths)\n",
    "\n",
    "print('Max Seq Length', max_user_length)\n",
    "# print(user_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "for data_type in ['train', 'test']:\n",
    "    x[data_type] = sequence.pad_sequences(x[data_type], maxlen=max_user_length)\n",
    "    y[data_type] = sequence.pad_sequences(y[data_type], maxlen=max_user_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Approaches (Without Events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Dense, TimeDistributed, Masking, SimpleRNN, Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_events = Input(shape=(max_user_length, 1))\n",
    "input_deltas = Input(shape=(max_user_length, 1))\n",
    "\n",
    "inputs = Concatenate(axis=2)([input_events, input_deltas])\n",
    "inputs = Masking(mask_value=0.)(inputs)\n",
    "\n",
    "rnn = SimpleRNN(50, activation='tanh', return_sequences=True)(inputs)\n",
    "\n",
    "output_events = TimeDistributed(Dense(num_events, activation='softmax'), name='events')(rnn)\n",
    "\n",
    "output_deltas = TimeDistributed(Dense(15, activation='relu'))(rnn)\n",
    "output_deltas = TimeDistributed(Dense(1, activation='sigmoid'), name='time')(output_deltas)\n",
    "\n",
    "model = Model(inputs=[input_events, input_deltas], outputs=[output_events, output_deltas])\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy', 'mse'], optimizer='adam', metrics={'events': 'accuracy'})\n",
    "\n",
    "model.fit(x['train'], y['train'], validation_data=(x['test'], y['test']), batch_size=100, epochs=3)\n",
    "\n",
    "preds_train = model.predict(x['train'])\n",
    "preds_test = model.predict(x['test'])\n",
    "\n",
    "preds_train[1] *= max_delta\n",
    "preds_test[1] *= max_delta\n",
    "\n",
    "show_results_nn(preds_train, preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WTTE-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "y = {}\n",
    "\n",
    "for data_type in ['train', 'test']:\n",
    "    # x[data_type] = [seq[:-1] for seq in deltas[data_type]]\n",
    "    x[data_type] = [[1 for event in seq[:-1]] for seq in deltas[data_type]]\n",
    "    y[data_type] = [seq[1:] for seq in deltas[data_type]]\n",
    "\n",
    "    x[data_type] = np.asarray(x[data_type])\n",
    "    y[data_type] = np.asarray(y[data_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Seq Length 735\n"
     ]
    }
   ],
   "source": [
    "# Find the max user length\n",
    "user_lengths = [len(seq) for seq in deltas['train']] + [len(seq) for seq in deltas['test']]\n",
    "user_length_counts = {}\n",
    "for length in user_lengths:\n",
    "    if length not in user_length_counts:\n",
    "        user_length_counts[length] = 0\n",
    "        \n",
    "    user_length_counts[length] += 1\n",
    "user_lengths = list(set(user_lengths))\n",
    "user_lengths = sorted(user_lengths, reverse=True)\n",
    "max_user_length = max(user_lengths)\n",
    "\n",
    "print('Max Seq Length', max_user_length)\n",
    "# print(user_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "for data_type in ['train', 'test']:\n",
    "    x[data_type] = sequence.pad_sequences(x[data_type], maxlen=max_user_length)\n",
    "    y[data_type] = sequence.pad_sequences(y[data_type], maxlen=max_user_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "(5307, 735, 1)\n",
      "(5307, 735, 1)\n",
      "(5307, 735, 2)\n",
      "---\n",
      "(1326, 735, 1)\n",
      "(1326, 735, 1)\n",
      "(1326, 735, 2)\n"
     ]
    }
   ],
   "source": [
    "for data_type in ['train', 'test']:\n",
    "    # x[data_type] = np.expand_dims(x[data_type], axis=2)\n",
    "    # y[data_type] = np.expand_dims(y[data_type], axis=2)\n",
    "    \n",
    "    y_shape = y[data_type].shape\n",
    "    y_add = np.zeros((y_shape[0], y_shape[1], 1))\n",
    "    y[data_type] = np.concatenate((y[data_type], y_add), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5307, 735, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in a censored value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def _keras_unstack_hack(ab):\n",
    "    \"\"\"Implements tf.unstack(y_true_keras, num=2, axis=-1).\n",
    "       Keras-hack adopted to be compatible with theano backend.\n",
    "    \"\"\"\n",
    "    ndim = len(K.int_shape(ab))\n",
    "    if ndim == 0:\n",
    "        print('can not unstack with ndim=0')\n",
    "    else:\n",
    "        a = ab[..., 0]\n",
    "        b = ab[..., 1]\n",
    "    return a, b\n",
    "\n",
    "def weibull_loss_discrete(y_true, y_pred, name=None):\n",
    "    \"\"\"calculates a keras loss op designed for the sequential api.\n",
    "    \n",
    "        Discrete log-likelihood for Weibull hazard function on censored survival data.\n",
    "        For math, see \n",
    "        https://ragulpr.github.io/assets/draft_master_thesis_martinsson_egil_wtte_rnn_2016.pdf (Page 35)\n",
    "        \n",
    "        Args:\n",
    "            y_true: tensor with last dimension having length 2\n",
    "                with y_true[:,...,0] = time to event, \n",
    "                     y_true[:,...,1] = indicator of not censored\n",
    "                \n",
    "            y_pred: tensor with last dimension having length 2 \n",
    "                with y_pred[:,...,0] = alpha, \n",
    "                     y_pred[:,...,1] = beta\n",
    "\n",
    "        Returns:\n",
    "            A positive `Tensor` of same shape as input\n",
    "            \n",
    "    \"\"\"    \n",
    "    y,u = _keras_unstack_hack(y_true)\n",
    "    a,b = _keras_unstack_hack(y_pred)\n",
    "\n",
    "    hazard0 = K.pow((y + 1e-35) / a, b)\n",
    "    hazard1 = K.pow((y + 1.0) / a, b)\n",
    "    \n",
    "    loglikelihoods = u * K.log(K.exp(hazard1 - hazard0) - 1.0) - hazard1\n",
    "    loss = -1 * K.mean(loglikelihoods)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def output_lambda(x, init_alpha=1.0, max_beta_value=5.0, max_alpha_value=None):\n",
    "    \"\"\"Elementwise (Lambda) computation of alpha and regularized beta.\n",
    "\n",
    "        Alpha: \n",
    "        (activation) \n",
    "        Exponential units seems to give faster training than \n",
    "        the original papers softplus units. Makes sense due to logarithmic\n",
    "        effect of change in alpha. \n",
    "        (initialization) \n",
    "        To get faster training and fewer exploding gradients,\n",
    "        initialize alpha to be around its scale when beta is around 1.0,\n",
    "        approx the expected value/mean of training tte. \n",
    "        Because we're lazy we want the correct scale of output built\n",
    "        into the model so initialize implicitly; \n",
    "        multiply assumed exp(0)=1 by scale factor `init_alpha`.\n",
    "\n",
    "        Beta: \n",
    "        (activation) \n",
    "        We want slow changes when beta-> 0 so Softplus made sense in the original \n",
    "        paper but we get similar effect with sigmoid. It also has nice features.\n",
    "        (regularization) Use max_beta_value to implicitly regularize the model\n",
    "        (initialization) Fixed to begin moving slowly around 1.0\n",
    "\n",
    "        Assumes tensorflow backend.\n",
    "\n",
    "        Args:\n",
    "            x: tensor with last dimension having length 2\n",
    "                with x[...,0] = alpha, x[...,1] = beta\n",
    "\n",
    "        Usage:\n",
    "            model.add(Dense(2))\n",
    "            model.add(Lambda(output_lambda, arguments={\"init_alpha\":100., \"max_beta_value\":2.0}))\n",
    "        Returns:\n",
    "            A positive `Tensor` of same shape as input\n",
    "    \"\"\"\n",
    "    a, b = _keras_unstack_hack(x)\n",
    "\n",
    "    # Implicitly initialize alpha:\n",
    "    if max_alpha_value is None:\n",
    "        a = init_alpha * K.exp(a)\n",
    "    else:\n",
    "        a = init_alpha * K.clip(x=a, min_value=K.epsilon(),\n",
    "                                max_value=max_alpha_value)\n",
    "\n",
    "    m = max_beta_value\n",
    "    if m > 1.05:  # some value >>1.0\n",
    "        # shift to start around 1.0\n",
    "        # assuming input is around 0.0\n",
    "        _shift = np.log(m - 1.0)\n",
    "\n",
    "        b = K.sigmoid(b - _shift)\n",
    "    else:\n",
    "        b = K.sigmoid(b)\n",
    "\n",
    "    # Clipped sigmoid : has zero gradient at 0,1\n",
    "    # Reduces the small tendency of instability after long training\n",
    "    # by zeroing gradient.\n",
    "    b = m * K.clip(x=b, min_value=K.epsilon(), max_value=1. - K.epsilon())\n",
    "\n",
    "    x = K.stack([a, b], axis=-1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n_timesteps, every_nth,n_repeats,noise_level,n_features,use_censored = True):\n",
    "    def get_equal_spaced(n, every_nth):\n",
    "        # create some simple data of evenly spaced events recurring every_nth step\n",
    "        # Each is on (time,batch)-format\n",
    "        events = np.array([np.array(range(n)) for _ in range(every_nth)])\n",
    "        events = events + np.array(range(every_nth)).reshape(every_nth, 1) + 1\n",
    "\n",
    "        tte_actual = every_nth - 1 - events % every_nth\n",
    "\n",
    "        was_event = (events % every_nth == 0) * 1.0\n",
    "        was_event[:, 0] = 0.0\n",
    "\n",
    "        events = tte_actual == 0\n",
    "\n",
    "        is_censored = (events[:, ::-1].cumsum(1)[:, ::-1] == 0) * 1\n",
    "        tte_censored = is_censored[:, ::-1].cumsum(1)[:, ::-1] * is_censored\n",
    "        tte_censored = tte_censored + (1 - is_censored) * tte_actual\n",
    "\n",
    "        events = np.copy(events.T * 1.0)\n",
    "        tte_actual = np.copy(tte_actual.T * 1.0)\n",
    "        tte_censored = np.copy(tte_censored.T * 1.0)\n",
    "        was_event = np.copy(was_event.T * 1.0)\n",
    "        not_censored = 1 - np.copy(is_censored.T * 1.0)\n",
    "\n",
    "        return tte_censored, not_censored, was_event, events, tte_actual\n",
    "    \n",
    "    tte_censored,not_censored,was_event,events,tte_actual = get_equal_spaced(n=n_timesteps,every_nth=every_nth)\n",
    "\n",
    "    # From https://keras.io/layers/recurrent/\n",
    "    # input shape rnn recurrent if return_sequences: (nb_samples, timesteps, input_dim)\n",
    "\n",
    "    u_train      = not_censored.T.reshape(n_sequences,n_timesteps,1)\n",
    "    x_train      = was_event.T.reshape(n_sequences,n_timesteps,1)\n",
    "    tte_censored = tte_censored.T.reshape(n_sequences,n_timesteps,1)\n",
    "    y_train      = np.append(tte_censored,u_train,axis=2) # (n_sequences,n_timesteps,2)\n",
    "\n",
    "    u_test       = np.ones(shape=(n_sequences,n_timesteps,1))\n",
    "    x_test       = np.copy(x_train)\n",
    "    tte_actual   = tte_actual.T.reshape(n_sequences,n_timesteps,1)\n",
    "    y_test       = np.append(tte_actual,u_test,axis=2) # (n_sequences,n_timesteps,2)\n",
    "\n",
    "    if not use_censored:\n",
    "        x_train = np.copy(x_test)\n",
    "        y_train = np.copy(y_test)\n",
    "    # Since the above is deterministic perfect fit is feasible. \n",
    "    # More noise->more fun so add noise to the training data:\n",
    "    \n",
    "    x_train = np.tile(x_train.T,n_repeats).T\n",
    "    y_train = np.tile(y_train.T,n_repeats).T\n",
    "\n",
    "    # Try with more than one feature TODO\n",
    "    x_train_new = np.zeros([x_train.shape[0],x_train.shape[1],n_features])\n",
    "    x_test_new = np.zeros([x_test.shape[0],x_test.shape[1],n_features])\n",
    "    for f in range(n_features):\n",
    "        x_train_new[:,:,f] = x_train[:,:,0]\n",
    "        x_test_new[:,:,f]  = x_test[:,:,0]\n",
    "        \n",
    "    x_train = x_train_new\n",
    "    x_test  = x_test_new\n",
    "    \n",
    "    # xtrain is signal XOR noise with probability noise_level\n",
    "    noise = np.random.binomial(1,noise_level,size=x_train.shape)\n",
    "    x_train = x_train+noise-x_train*noise\n",
    "    return y_train,x_train, y_test,x_test,events\n",
    "\n",
    "\n",
    "n_timesteps    = 200\n",
    "n_sequences = every_nth = 80\n",
    "n_features = 1\n",
    "n_repeats = 1000\n",
    "noise_level = 0.005\n",
    "use_censored = True\n",
    "\n",
    "y_train,x_train, y_test,x_test,events = get_data(n_timesteps, every_nth,n_repeats,noise_level,n_features,use_censored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from keras.optimizers import RMSprop,adam\n",
    "from keras.callbacks import History, TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5307, 735)\n",
      "(80000, 200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y['train'].shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tte_mean_train = np.nanmean(y_train[:,:,0])\n",
    "init_alpha = -1.0/np.log(1.0-1.0/(tte_mean_train+1.0) )\n",
    "init_alpha = init_alpha/np.nanmean(y_train[:,:,1]) # use if lots of censoring\n",
    "print('init_alpha: ', init_alpha)\n",
    "\n",
    "# Start building the model\n",
    "model = Sequential()\n",
    "#model.add(TimeDistributed(Dense(2), input_shape=(None, n_features)))\n",
    "model.add(GRU(1, input_shape=(n_timesteps, n_features),activation='tanh',return_sequences=True))\n",
    "\n",
    "model.add(TimeDistributed(Dense(2)))\n",
    "model.add(TimeDistributed(Lambda(output_lambda, arguments={\"init_alpha\":init_alpha, \n",
    "                                               \"max_beta_value\":4.0})))\n",
    "\n",
    "model.compile(loss=weibull_loss_discrete, optimizer=adam(lr=.01))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/python35/lib/python3.5/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_alpha:  inf\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_2 (GRU)                  (None, 735, 1)            9         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 735, 2)            4         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 735, 2)            0         \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tte_mean_train = np.nanmean(y['train'][:,:,0])\n",
    "init_alpha = -1.0/np.log(1.0-1.0/(tte_mean_train+1.0) )\n",
    "init_alpha = init_alpha/np.nanmean(y['train'][:,:,1]) # use if lots of censoring\n",
    "print('init_alpha: ', init_alpha)\n",
    "\n",
    "# Start building the model\n",
    "model = Sequential()\n",
    "#model.add(TimeDistributed(Dense(2), input_shape=(None, n_features)))\n",
    "model.add(GRU(1, input_shape=(x['train'].shape[1], n_features),activation='tanh',return_sequences=True))\n",
    "\n",
    "model.add(TimeDistributed(Dense(2)))\n",
    "model.add(TimeDistributed(Lambda(output_lambda, arguments={\"init_alpha\":init_alpha, \n",
    "                                               \"max_beta_value\":4.0})))\n",
    "\n",
    "model.compile(loss=weibull_loss_discrete, optimizer=adam(lr=.01))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      " - 12s - loss: nan\n",
      "Epoch 2/80\n",
      " - 12s - loss: nan\n",
      "Epoch 3/80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-fcaf7c64d29d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#           validation_data=(x_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m          )\n",
      "\u001b[0;32m//anaconda/envs/python35/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m//anaconda/envs/python35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m//anaconda/envs/python35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x['train'], y['train'],\n",
    "          epochs=80, \n",
    "          batch_size=x['train'].shape[0]//10, \n",
    "          verbose=2, \n",
    "#           validation_data=(x_test, y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preds = model.predict(x_test)\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           epochs=80, \n",
    "#           batch_size=x_train.shape[0]//10, \n",
    "#           verbose=2, \n",
    "#           validation_data=(x_test, y_test)\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
